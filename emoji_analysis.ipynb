{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import pymongo\n",
    "from scipy import spatial\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import os.path as path\n",
    "from sklearn import cross_validation\n",
    "from textblob import TextBlob, Word, WordList\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to add two word vectors together\n",
    "def matrix_sum(lst):\n",
    "    if len(lst) > 0:\n",
    "        return np.array([sum(x) for x in zip(*[w2v[i] for i in lst])])\n",
    "    else:\n",
    "        return np.zeros((300,), dtype=np.int)\n",
    "\n",
    "\n",
    "### A series of operations to clean up the Twitter text    \n",
    "def preprocess(lst):\n",
    "    stop = stopwords.words('english')\n",
    "    punctuation = ['.', ',', '\"', ':', ';', '\"\"', '!', '?']\n",
    "    stop.append('rt')\n",
    "    stoplist = stop\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    lst = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', lst) \n",
    "\n",
    "    lst = tknzr.tokenize(lst)\n",
    "    lst = ' '.join(lst)\n",
    "\n",
    "    text = [word for word in lst.lower().split() if word not in stoplist]\n",
    "    text = [word for word in text if word not in punctuation]\n",
    "    \n",
    "    for i in text:\n",
    "        if '@' in i:\n",
    "            text.remove(i)\n",
    "#         elif '@' in w:\n",
    "#             i.remove(w)\n",
    "        else:\n",
    "            next\n",
    "            \n",
    "    return text\n",
    "\n",
    "\n",
    "### Measures the similarity between emoji vectors\n",
    "def emoji_similarity(lst):\n",
    "    if len(lst) > 0:\n",
    "        cosine_test_vec = [w2v[i] for i in lst]\n",
    "        return np.mean(cosine_similarity(cosine_test_vec))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Spins up the Mongo client to query for tweets from the emojidb database\n",
    "client = MongoClient('localhost', 27017, connect=False)\n",
    "emojidb = client.emojidb\n",
    "emojitweets = emojidb.emojitweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Returns the text of the Twitter corpus and places into a dataframe\n",
    "cursor = emojitweets.find({}, { 'text': 1})\n",
    "df_tweets = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### A number of operations to build the analytical dataframe:\n",
    "\n",
    "### Converts tweets to strings, in the event they are not already\n",
    "df_tweets['text'] = df_tweets['text'].astype(str)\n",
    "\n",
    "### Cleans up the text using our preprocess function\n",
    "df_tweets['words'] = df_tweets['text'].map(lambda x: preprocess(x))\n",
    "\n",
    "### Pulls out the emoji used in a given tweet \n",
    "df_tweets['tweet_emoji'] = df_tweets['words'].map(lambda x: [i for i in x if i in emoji_list])\n",
    "\n",
    "### Pulls out the text (no emoji) from each tweet\n",
    "df_tweets['tweet_text'] = df_tweets['words'].map(lambda x: [i for i in x if i not in emoji_list])\n",
    "\n",
    "### Counts the number of total emoji used\n",
    "df_tweets['emoji_used'] = df_tweets['tweet_emoji'].map(lambda x: len(x))\n",
    "\n",
    "### Counts the number of unique emoji used\n",
    "df_tweets['unique_emoji_used'] = df_tweets['tweet_emoji'].map(lambda x: len(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trains Word2Vec model on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creates list of lemmatized and cleaned tweets from our dataframe\n",
    "vec_tweet_list = list(df_tweets['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Trains Word2Vec model on tweets with 300 dimensions\n",
    "w2v = models.Word2Vec(vec_tweet_list, size=300, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Saves model\n",
    "w2v.save('w2v_01.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses output of Word2Vec model to measure emoji complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculates aggregate text vector\n",
    "df_tweets['text_sum'] = df_tweets['tweet_text'].map(matrix_sum)\n",
    "\n",
    "### Calculates aggregate emoji vector\n",
    "df_tweets['emoji_sum'] = df_tweets['tweet_emoji'].map(matrix_sum)\n",
    "\n",
    "### Calculates similarity of emoji used\n",
    "df_tweets['emoji_similarity'] = df_tweets['tweet_emoji'].map(emoji_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Because calculating cosine similarities on 300-dimension vectors 1 million times is resource-intensive,\n",
    "I split out the vectors into lists to make it a little more efficient\n",
    "\"\"\"\n",
    "vec_emoji_list = list(df_tweets['emoji_sum'])\n",
    "vec_text_list = list(df_tweets['text_sum'])\n",
    "vec_cosine_list = list(zip(vec_emoji_list, vec_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculates the cosine similarity between emoji and text,\n",
    "### puts it into a list so we can add it back to our dataframe\n",
    "cosine_sim = []\n",
    "\n",
    "for i in vec_cosine_list:\n",
    "    sim = 1 - spatial.distance.cosine(i[0], i[1])\n",
    "#     sim  = cosine_similarity(i[0], i[1])[0]\n",
    "    cosine_sim.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Makes our cosine similarity list an array\n",
    "m = np.asarray(cosine_sim)\n",
    "\n",
    "### Adds cosine similarities back into df_tweets\n",
    "df_tweets[\"emoji_text_similarity\"] = m[df_tweets.index]\n",
    "\n",
    "### Replaces NaN's\n",
    "df_tweets[\"emoji_text_similarity\"] = df_tweets[\"emoji_text_similarity\"].replace(np.nan, 0)\n",
    "\n",
    "### Normalizes the similarities on a scale from 0 to 1, rather than -1 to 1\n",
    "df_tweets[\"emoji_text_similarity\"] = df_tweets[\"emoji_text_similarity\"].map(lambda x: (x + 1) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the simple dumb model of emoji complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tweets['complexity'] = (df_tweets['unique_emoji_used'] / df_tweets['emoji_used']) - df_tweets['emoji_similarity'] - df_tweets['emoji_text_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Normalizes complexity on a scale of 0 to 1 using the max and min final complexity values\n",
    "df_tweets['complexity'] = df_tweets['complexity'].map(lambda x: (x - (-1.806203)) / ((0.235635) - (-1.806203)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builds the network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Builds out the JSON of emoji cross-usage for network analysis\n",
    "df_network = df_tweets[df_tweets['unique_emoji_used'] > 2]\n",
    "df_network.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Creates a list of emojis in the corpus by tweet to begin network pairings\n",
    "tweet_emoji_list = list(df_network['tweet_emoji'])\n",
    "tweet_emoji_list = [' '.join(x) for x in tweet_emoji_list]\n",
    "tweet_emoji_list = list(filter(None, tweet_emoji_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we cycle through each emoji, see if that emoji is in each tweet, and if it is, append it to a list of emoji used\n",
    "with the search emoji. This is to build a \"network\" of emoji that are used with one another.\n",
    "\"\"\"\n",
    "\n",
    "emoji_network_json = []\n",
    "\n",
    "for e in emoji_list:\n",
    "    emoji_dict = {}\n",
    "    emoji_node_links = []\n",
    "    for m in tweet_emoji_list:\n",
    "        if e in m:\n",
    "            m_list = m.split()\n",
    "            m_list = [i for i in m_list if i in keep_emoji]\n",
    "            emoji_node_links.extend(m_list)\n",
    "\n",
    "        else:\n",
    "            next\n",
    "\n",
    "        emoji_node_links = list(set(emoji_node_links))\n",
    "\n",
    "    if len(emoji_node_links) > 1:    \n",
    "        emoji_dict[\"name\"] = e\n",
    "        emoji_dict['size'] = 2000\n",
    "        emoji_dict['imports'] = emoji_node_links\n",
    "        emoji_network_json.append(emoji_dict)\n",
    "    else:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Saves the full JSON of emoji couplings\n",
    "with open('emoji_network.json', 'w', encoding='string_escape') as fp:\n",
    "    json.dump(emoji_network, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Saves the top 100 emoji most used with other emoji for a cleaner visualization\n",
    "emoji_network_sorted = sorted(emoji_network_json, key=lambda k:len(k), reverse = True)\n",
    "emoji_network = emoji_network_sorted[:100]\n",
    "keep_emoji = [i['name'] for i in emoji_network]\n",
    "\n",
    "for i in emoji_network:\n",
    "    for n in i['imports']:\n",
    "        if n not in keep_emoji:\n",
    "            i['imports'].remove(n)\n",
    "        else:\n",
    "            next"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
